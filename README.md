# QuantCacheÔºöAdaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation

Junyi Wu, [Zhiteng Li](https://zhitengli.github.io), [Zheng Hui](https://zheng222.github.io/), [Yulun Zhang](http://yulunzhang.com/), [Linghe Kong](https://www.cs.sjtu.edu.cn/~linghe.kong/) and [Xiaokang Yang](https://scholar.google.com/citations?user=yDEavdMAAAAJ)

[[arXiv](
)] [[supplementary material]()]




#### üî•üî•üî• News

- **2025-03-09:** This repo is released.

---

> **Abstract:** Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72√ó on Open-Sora with minimal loss in generation quality. Extensive experiments across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. The code and models will be available at https://github.com/JunyiWuCode/QuantCache.

## ‚öíÔ∏è TODO
 
* [ ] Complete this repository

## üîó Contents

- [ ] [Performance and Efficiency](#performance-and-efficiency)
- [ ] [Results](#-results)
- [ ] [Citation](#citation)
- [ ] [Acknowledgements](#-acknowledgements)
- [ ] 
## ‚ú® Performance and Efficiency
<a id="performance-and-efficiency"></a>

## üîé Results
## Citation

If you find the code helpful in your research or work, please cite the following paper.

```

```

## üí° Acknowledgements

This work is released under the Apache 2.0 license.
The codes are based on [SVD-LLM](https://github.com/AIoT-MLSys-Lab/SVD-LLM). Please also follow their licenses. Thanks for their awesome works.


